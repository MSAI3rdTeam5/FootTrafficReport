from ultralytics import YOLO
from datetime import datetime
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
 
import os
import torch
import cv2
import random
import pandas as pd
import aiohttp
import asyncio
import numpy as np
import onnxruntime as ort
 
app = FastAPI()
 
# í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬í•˜ëŠ” ìš”ì²­ bodyì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸
class DetectionRequest(BaseModel):
    cctv_url: str
    cctv_id: str
 
class ONNXModel:
    def __init__(self, model_path):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]

    def predict(self, input_data):
        # âœ… input_dataê°€ ë¬¸ìì—´ì´ë©´ float ë³€í™˜ ì‹œë„
        if isinstance(input_data, str):
            try:
                input_data = float(input_data)
            except ValueError:
                # ë¬¸ìì—´ì¸ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
                print(f"Invalid input data: {input_data} (Expected float)")
                return None

        # âœ… input_dataë¥¼ float32 ë°°ì—´ë¡œ ë³€í™˜
        input_data = np.array([input_data], dtype=np.float32)

        # âœ… ONNX ëª¨ë¸ ì‹¤í–‰
        results = self.session.run(self.output_names, {self.input_name: input_data})

        return results



    def process_output(self, results):
        predictions = results[0]  # ì²« ë²ˆì§¸ ì¶œë ¥ ì„ íƒ
        
        if isinstance(predictions, np.ndarray) and predictions.shape[1] > 1:
            # ì—¬ëŸ¬ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ì²˜ë¦¬
            labels = ['Male', 'Female', 'AgeLess18', 'Age18to60', 'AgeOver60']
            return {label: float(predictions[0][i]) for i, label in enumerate(labels)}
        else:
            print("Unexpected output format:", predictions)
            return {'Male': 0.0, 'Female': 0.0, 'AgeLess18': 0.0, 'Age18to60': 0.0, 'AgeOver60': 0.0}


# Initialize PersonTracker with model and configuration
# ëª¨ë¸ ë° êµ¬ì„±ìœ¼ë¡œ PersonTracker ì´ˆê¸°í™”
class PersonTracker:
    def __init__(self, model_path, onnx_model_path, result_dir='../outputs/results/', tracker_config="../data/config/botsort.yaml", conf=0.5, device=None,
                 iou=0.5, img_size=(720, 1080), output_dir='../outputs/results_video'):
        self.device = device if device else ('cuda:0' if torch.cuda.is_available() else 'cpu')
        self.model = YOLO(model_path)
        self.result_dir = result_dir
        self.tracker_config = tracker_config
        self.conf = conf
        self.iou = iou
        self.img_size = img_size
        self.output_dir = output_dir
        self.color_map = {}
        self.frames = []
        self.boxes = []
        self.detected_ids = set()
        self.captured_objects = set()
        self.detected_ids_full_entry = set()
        self.onnx_model = ONNXModel(onnx_model_path)
   
    def is_fully_inside_frame(self, x1, y1, x2, y2, frame_shape):  # ì¶”ê°€ëœ ì½”ë“œ
        h, w, _ = frame_shape
        return x1 >= 0 and y1 >= 0 and x2 <= w and y2 <= h
 
    def save_full_frame(self, frame, obj_id):  # ì¶”ê°€ëœ ì½”ë“œ
        save_dir = "../outputs/full_frames/"
        os.makedirs(save_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
        file_name = f"{save_dir}{timestamp}_ID{obj_id}.jpg"
        cv2.imwrite(file_name, frame)
        print(f"[INFO] Full frame saved: {file_name}")
 
    # Generate a unique color for each object ID
    # ê° ê°ì²´ IDì— ëŒ€í•œ ê³ ìœ í•œ ìƒ‰ìƒ ìƒì„±
    def generate_color(self, obj_id):
        if obj_id not in self.color_map:
            self.color_map[obj_id] = [random.randint(0, 255) for _ in range(3)]
        return self.color_map[obj_id]
    
    # ì–¼êµ´ ë¸”ëŸ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³„ì‚°
    def estimate_face_area(self, keypoints, box):
        face_keypoints_indices = [0, 1, 2, 3, 4]
        face_keypoints = keypoints[face_keypoints_indices]

        if face_keypoints.shape[1] == 2:
            valid_points = face_keypoints
        elif face_keypoints.shape[1] == 3:
            valid_points = face_keypoints[face_keypoints[:, 2] > 0.3][:, :2]
        else:
            return None

        if len(valid_points) >= 4:
            x_min, y_min = np.maximum(np.min(valid_points, axis=0).astype(int), [box[0], box[1]])
            x_max, y_max = np.minimum(np.max(valid_points, axis=0).astype(int), [box[2], box[3]])

            width = (x_max - x_min) * 20
            height = (y_max - y_min) * 10
            x_min = max(box[0], x_min - int(width * 0.1))
            y_min = max(box[1], y_min - int(height * 0.1))
            x_max = min(box[2], x_max + int(width * 0.1))
            y_max = min(box[3], y_max + int(height * 0.1))

            return x_min, y_min, x_max, y_max

        return None
    # ì–¼êµ´ ë¸”ëŸ¬
    def apply_face_blur(self, frame, face_area):
        if face_area is not None:
            x_min, y_min, x_max, y_max = face_area

            if x_max > x_min and y_max > y_min:
                face_roi = frame[y_min:y_max, x_min:x_max]
                blurred_roi = cv2.GaussianBlur(face_roi, (25, 25), 0)
                frame[y_min:y_max, x_min:x_max] = blurred_roi
        return frame
 
    # Detect and track people in the video stream
    # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì‚¬ëŒì„ ê°ì§€í•˜ê³  ì¶”ì 
    async def detect_and_track(self, source, cctv_id):
        results = self.model.track(
            source, show=False, stream=True, tracker=self.tracker_config, conf=self.conf,
            device=self.device, iou=self.iou, stream_buffer=True, classes=[0], imgsz=self.img_size
        )

        for result in results:
            original_frame = result.orig_img.copy()  # ì›ë³¸ í”„ë ˆì„ ì €ì¥
            display_frame = original_frame.copy()    # ë””ìŠ¤í”Œë ˆì´ìš© í”„ë ˆì„
            boxes = result.boxes
            keypoints_data = result.keypoints.data.cpu().numpy()

            self.frames.append(display_frame)
            self.boxes.append(boxes)

            tasks = []
            new_object_detected = False

            for box, kpts in zip(boxes, keypoints_data):
                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                if box.id is not None:
                    obj_id = int(box.id)
                else:
                    continue

                color = self.generate_color(obj_id)

                if obj_id not in self.detected_ids:
                    self.detected_ids.add(obj_id)
                    cropped_path = self.save_cropped_person(original_frame, x1, y1, x2, y2, obj_id)  # ì›ë³¸ í”„ë ˆì„ì—ì„œ í¬ë¡­
                    tasks.append(self.process_person(obj_id, cropped_path, cctv_id))
                    new_object_detected = True

                face_area = self.estimate_face_area(kpts, [x1, y1, x2, y2])
                if face_area:
                    display_frame = self.apply_face_blur(display_frame, face_area)

                cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(display_frame, f"ID: {obj_id}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            await asyncio.gather(*tasks)

            if new_object_detected:
                self.save_full_frame(original_frame, obj_id)  # ì›ë³¸ í”„ë ˆì„ ì €ì¥

            cv2.imshow("Person Tracking", display_frame)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                while True:
                    if cv2.waitKey(1) & 0xFF == ord('s'):
                        break

        cv2.destroyAllWindows()
        self.save_blurred_video_prompt()
 
    # Process detected person using ONNX
    # ONNXë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì§€ëœ ì‚¬ëŒ ì²˜ë¦¬
    async def process_person(self, obj_id, cropped_path, cctv_id):
        image = cv2.imread(cropped_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (224, 224))
        image = image.astype(np.float32) / 255.0
        image = np.transpose(image, (2, 0, 1))
        image = np.expand_dims(image, axis=0)
        
        # ONNX ì¶”ë¡  ì‹¤í–‰
        ort_inputs = {self.onnx_model.input_name: image}
        ort_outputs = self.onnx_model.session.run(self.onnx_model.output_names, ort_inputs)
        
        # ì˜ˆì¸¡ ì²˜ë¦¬
        predictions = self.onnx_model.predict(ort_outputs[0])
        
        print(f"ID: {obj_id}")
        for label, value in predictions.items():
            print(f"{label}: {value:.4f}")
        
        # ì„œë²„ë¡œ ë°ì´í„° ì „ì†¡ (ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ genderì™€ ageë¥¼ ì „ì†¡)
        gender = max(['Male', 'Female'], key=lambda x: predictions[x])
        age = max(['AgeLess18', 'Age18to60', 'AgeOver60'], key=lambda x: predictions[x])
        
        await self.send_data_to_server(obj_id, gender, age, cctv_id)
        if isinstance(ort_outputs[0], str):
            print(f"ğŸš¨ Warning: ONNX output is a string: {ort_outputs[0]}")


 
    # Send analysis results to the backend server
    # ë¶„ì„ ê²°ê³¼ë¥¼ ë°±ì—”ë“œ ì„œë²„ë¡œ ì „ì†¡
    async def send_data_to_server(self, obj_id, gender, age, cctv_id):
        """ë°±ì—”ë“œ ì„œë²„ë¡œ ë¶„ì„ ê²°ê³¼ ì „ì†¡"""
        url = "https://msteam5iseeu.ddns.net/api/cctv_data"
        data = {
            "cctv_id": cctv_id,
            "detected_time": datetime.now().isoformat(),
            "person_label": str(obj_id),
            "gender": gender,
            "age": age
        }
 
        session = aiohttp.ClientSession()
        try:
            async with session.post(url, json=data) as response:
                print(await response.json())
        except aiohttp.ClientError as e:
            print(f"âš ï¸ Failed to connect to server: {e}")
        finally:
            await session.close()
 
    # Save cropped image of detected person
    # ê°ì§€ëœ ì‚¬ëŒì˜ í¬ë¡­ëœ ì´ë¯¸ì§€ ì €ì¥    
    def save_cropped_person(self, frame, x1, y1, x2, y2, obj_id, save_dir="../outputs/cropped_people/"):
        os.makedirs(save_dir, exist_ok=True)
        file_name = f"{save_dir}person_{obj_id}.jpg"
        cv2.imwrite(file_name, frame[y1:y2, x1:x2])
        return file_name
   
    # Prompt user to save blurred video
    # ì‚¬ìš©ìì—ê²Œ ë¸”ëŸ¬ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ ì €ì¥ ì—¬ë¶€ ë¬»ê¸°
    def save_blurred_video_prompt(self):
        save_input = input("Do you want to save the blurred video? (y/n): ").strip().lower()
        if save_input == 'y':
            self.save_blurred_video()
        elif save_input == 'n':
            print("Video not saved.")
        else:
            print("Invalid input. Please enter 'y' or 'n'.")
            self.save_blurred_video_prompt()
 
    # Save video with blurred faces
    # ì–¼êµ´ì´ ë¸”ëŸ¬ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ ì €ì¥
    def save_blurred_video(self):
        os.makedirs(self.output_dir, exist_ok=True)
        video_name = datetime.now().strftime("%Y-%m-%d-%H-%M-%S") + "_blurred.webm"
        output_path = os.path.join(self.output_dir, video_name)
       
        fourcc = cv2.VideoWriter_fourcc(*'VP80')
        height, width, _ = self.frames[0].shape
        out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))
       
        for frame, boxes in zip(self.frames, self.boxes):
            for box in boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                roi = frame[y1:y2, x1:x2]
                blurred_roi = cv2.GaussianBlur(roi, (15, 15), 0)
                frame[y1:y2, x1:x2] = blurred_roi
 
            out.write(frame)
 
        out.release()
        print(f"Blurred video saved at {output_path}")

'''
Test code í• ë•ŒëŠ” __name__ == "__main__"ìœ¼ë¡œ ì‹¤í–‰ (detect_people í•¨ìˆ˜ëŠ” ì£¼ì„ ì²˜ë¦¬)
ì›¹ìœ¼ë¡œ í˜¸ì¶œí•´ì„œ ì‹¤ì œ cctvì—ì„œ ì‹¤í–‰í• ë•ŒëŠ” detect_peopleë¡œ ì‹¤í–‰ (__name__ == "__main__" ì£¼ì„ ì²˜ë¦¬)
'''
# # ì›¹ìœ¼ë¡œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜
# @app.post("/detect") 
# async def detect_people(request: DetectionRequest):
#     try:
#         tracker = PersonTracker(
#             model_path='FootTrafficReport/people-detection/model/yolo11n-pose.pt'
#         )
#         result = await tracker.detect_and_track(source=request.cctv_url, cctv_id=request.cctv_id)
#         return result
    
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8500)


#Testí• ë•Œ í•˜ëŠ” ì‘ì—… (cctv_idëŠ” ì„ì˜ë¡œ ì„¤ì •)
if __name__ == '__main__':
    source = "../data/videos/07_cam.mp4"
    yolo_model_path = '../model/yolo11n-pose.pt'
    onnx_model_path = '../model/model.onnx'
    tracker = PersonTracker(model_path=yolo_model_path, onnx_model_path=onnx_model_path)
    asyncio.run(tracker.detect_and_track(source=source, cctv_id=1))
