from ultralytics import YOLO
from datetime import datetime
import os
import torch
import cv2
import random
import pandas as pd
import requests

# Azure Custom Vision API í˜¸ì¶œì„ ìœ„í•œ í´ë˜ìŠ¤
class AzureAPI:
    def __init__(self):
        self.url = "https://cvteam5-prediction.cognitiveservices.azure.com/customvision/v3.0/Prediction/6bf7f6a6-8f58-48ef-a6ad-3c1cf2d37ced/classify/iterations/Iteration2/image"
        self.headers = {
            "Prediction-Key": "8Icrrz5XXYWn6WOToZXmP6wWZ68hWOQDF4X6fOa3g8jPXc3zmrR0JQQJ99BAACYeBjFXJ3w3AAAIACOGhaam",
            "Content-Type": "application/octet-stream"
        }

    def analyze_image(self, image_path):
        """ Azure APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜ """
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
        response = requests.post(self.url, headers=self.headers, data=image_data)
        result = response.json()
        return self.normalize_predictions(result['predictions'])

    def normalize_predictions(self, predictions):
        """ ì„±ë³„ ë° ì—°ë ¹ ë¶„ì„ ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ì—¬ ë°˜í™˜ """
        gender_preds = {p['tagName']: p['probability'] * 100 for p in predictions if p['tagName'] in ['male', 'female']}
        age_preds = {p['tagName']: p['probability'] * 100 for p in predictions if p['tagName'] in ['adult', 'old', 'young']}
        
        def normalize_group(group_preds):
            total = sum(group_preds.values())
            return {k: (v/total)*100 for k, v in group_preds.items()} if total > 0 else group_preds
        
        return {**normalize_group(gender_preds), **normalize_group(age_preds)}


# ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ
CSV_PATH = "results/person_data.csv"
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)
if not os.path.exists(CSV_PATH):
    pd.DataFrame(columns=['ID', 'Gender', 'Age', 'Time']).to_csv(CSV_PATH, index=False)


# CSVì— ë°ì´í„° ì €ì¥
def save_to_csv(obj_id, gender, age):
    df = pd.read_csv(CSV_PATH)
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_data = pd.DataFrame([{
        'ID': obj_id, 'Gender': gender, 'Age': age, 'Time': current_time
    }])
    df = pd.concat([df, new_data], ignore_index=True)
    df.to_csv(CSV_PATH, index=False)

def save_cropped_image(self, frame, box, obj_id):
        """
        ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê°€ì¥ í¬ê²Œ ëŠ˜ì–´ë‚¬ì„ ë•Œ ìº¡ì²˜í•˜ì—¬ ì €ì¥
        """
        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
        width, height = x2 - x1, y2 - y1
        box_area = width * height  # ë°•ìŠ¤ ë©´ì  ê³„ì‚°

        # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ì„ í¬ë¡­í•œ ì´ë¯¸ì§€
        cropped_img = frame[y1:y2, x1:x2]

        # ë¹ˆ ì´ë¯¸ì§€ ë°©ì§€
        if cropped_img.size == 0:
            return

        # í•´ë‹¹ ê°ì²´ IDì˜ ê¸°ì¡´ ìµœëŒ€ í¬ê¸° ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì´ˆê¸°í™”)
        prev_max_area = self.max_box_sizes.get(obj_id, 0)

        # í˜„ì¬ ë°•ìŠ¤ê°€ ì´ì „ë³´ë‹¤ í¬ë©´ ì €ì¥
        if box_area > prev_max_area:
            self.max_box_sizes[obj_id] = box_area  # ìƒˆë¡œìš´ ìµœëŒ€ í¬ê¸° ì—…ë°ì´íŠ¸

            # ì €ì¥
            save_path = os.path.join(self.cropped_dir, f"ID_{obj_id}.jpg")
            cv2.imwrite(save_path, cropped_img)  # í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥

        # ì €ì¥ëœ ì´ë¯¸ì§€ ê²½ë¡œ ê´€ë¦¬
        self.saved_crops[obj_id] = save_path
    
def save_to_csv(obj_id, gender, gender_conf, age, age_conf):
    """ CSVì— ì˜ˆì¸¡ëœ ë°ì´í„° ì €ì¥ """
    df = pd.read_csv(CSV_PATH)
    new_data = pd.DataFrame([{
        'ID': obj_id, 'Gender': gender, 'Gender_Confidence': gender_conf,
        'Age': age, 'Age_Confidence': age_conf
    }])
    df = pd.concat([df, new_data], ignore_index=True)
    df.to_csv(CSV_PATH, index=False)
    
class PersonTracker:
    def __init__(self, model_path, result_dir='results/', tracker_config="config/botsort.yaml", conf=0.5, device=None,
                 iou=0.5, img_size=(720, 1080), output_dir='results_video'):
        self.device = device if device else ('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        self.model = YOLO(model_path)
        self.result_dir = result_dir
        self.tracker_config = tracker_config
        self.conf = conf
        self.iou = iou
        self.img_size = img_size
        self.output_dir = output_dir

        self.color_map = {}
        self.frames = []  # ì €ì¥í•  í”„ë ˆì„ì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸
        self.boxes = []  # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì €ì¥
        
        self.detected_ids = set()
        self.azure_api = AzureAPI()  # Azure API ê°ì²´ ìƒì„±

        self.max_box_sizes = {}  # ìµœëŒ€ ë°•ìŠ¤ í¬ê¸° ì €ì¥
        self.saved_crops = {}  # í¬ë¡­í•œ ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥

        # í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
        self.cropped_dir = "cropped_people"
        os.makedirs(self.cropped_dir, exist_ok=True)

    def create_result_file(self):
        folder_name = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")  
        result_file_path = os.path.join(self.result_dir, folder_name + ".txt") 
        os.makedirs(self.result_dir, exist_ok=True)  
        with open(result_file_path, 'w') as file:
            file.write(folder_name + "\n") 
        return result_file_path

    def generate_color(self, obj_id):
        # ê°ì²´ IDì— ë”°ë¼ ê³ ìœ  ìƒ‰ìƒì„ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ê¸°ì¡´ ìƒ‰ìƒ ë°˜í™˜)
        if obj_id not in self.color_map:
            self.color_map[obj_id] = [random.randint(0, 255) for _ in range(3)] 
        return self.color_map[obj_id] 
    
    def detect_and_track(self, source, show=True, logger=None):
        result_file = self.create_result_file()
        person_count = 0  
        previous_person_count = 0  

        # YOLO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì  ì‹œì‘
        results = self.model.track(
            source, show=False, stream=True, tracker=self.tracker_config, conf=self.conf,
            device=self.device, iou=self.iou, stream_buffer=True, classes=[0], imgsz=self.img_size
        )

        for i, result in enumerate(results):
            frame = result.orig_img  # í˜„ì¬ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
            boxes = result.boxes  # ë°•ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

            self.frames.append(frame)  # ì €ì¥í•  í”„ë ˆì„ ì¶”ê°€
            self.boxes.append(boxes)  # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì €ì¥

            try:
                 # ID ê°€ì ¸ì˜¤ê¸° (ì˜ˆì™¸ì²˜ë¦¬ ì¶”ê°€)
                try:
                    id_count = [int(box.id) for box in boxes]
                except:
                    id_count = []

                for box in boxes:
                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())  # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                    obj_id = int(box.id)  
                    color = self.generate_color(obj_id)   
                    
                    # Azure APIë¡œ ë¶„ì„í•  ë•Œ, ìƒˆë¡œ í¬ë¡­í•˜ëŠ” ëŒ€ì‹  ì €ì¥ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©
                if obj_id not in self.detected_ids and obj_id in self.saved_crops:
                    self.detected_ids.add(obj_id)  # ê°ì§€ëœ ID ì €ì¥
                    cropped_path = self.saved_crops[obj_id]  # ì €ì¥ëœ í¬ë¡­ ì´ë¯¸ì§€ ê²½ë¡œ
                    print(f"ğŸ“¤ Cropped Image Path: {cropped_path}")

                    # **Azure Custom Vision APIë¡œ ì „ì†¡**
                    predictions = self.azure_api.analyze_image(cropped_path)
                    print(predictions)  # ê²°ê³¼ ì¶œë ¥
                        
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                    # ê°ì²´ ID í…ìŠ¤íŠ¸ ì¶”ê°€
                    cv2.putText(frame, f"ID: {obj_id}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                max_id = max(id_count) if id_count else 0  # í˜„ì¬ê¹Œì§€ íƒì§€ëœ ê°ì²´ ìˆ˜

                # ì‚¬ëŒ ìˆ˜ ì¶œë ¥
                #cv2.putText(frame, f"Total Persons: {max_id}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

                # ì‚¬ëŒì´ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ê²°ê³¼ íŒŒì¼ì— ê¸°ë¡
                if max_id > person_count:
                    person_count = max_id
                    with open(result_file, 'a') as filewrite:
                        filewrite.write(f"Person count: {person_count}\n")

                    if logger:
                        logger.info(f"Person count: {person_count}")

            except Exception as e:
                print(f"Error: {e}")

            # í”„ë ˆì„ ë””ìŠ¤í”Œë ˆì´
            cv2.imshow("Person Tracking", frame)

            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):  # 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
                print("Exiting...")
                break
            elif key == ord('s'):  # 's'ë¥¼ ëˆ„ë¥´ë©´ ì˜ìƒ ì •ì§€
                print("Pausing video...")
                while True:
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('s'):  # 's'ë¥¼ ë‹¤ì‹œ ëˆŒëŸ¬ ì¬ê°œ
                        print("Resuming video...")
                        break
                    elif key == ord('q'):  # 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
                        print("Exiting...")
                        return

        cv2.destroyAllWindows()

        # ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì €ì¥ ì—¬ë¶€ ê²°ì •
        self.save_video_prompt()

    def save_video_prompt(self):
        save_input = input("Do you want to save the video? (y/n): ").strip().lower()
        if save_input == 'y':
            save_dir = "results_video"  
            os.makedirs(save_dir, exist_ok=True)  # í´ë” ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€)

            video_name = datetime.now().strftime("%Y-%m-%d-%H-%M-%S") + ".webm"
            video_path = os.path.join(self.result_dir, video_name)
            fourcc = cv2.VideoWriter_fourcc(*'VP80')  # WebM ì½”ë± (VP80), MP4 ì½”ë± (mp4v)
            height, width, _ = self.frames[0].shape
            out = cv2.VideoWriter(video_path, fourcc, 30, (width, height))

            for frame in self.frames:
                out.write(frame)

            out.release()
            print(f"Video saved at {video_path}")

            # ë°”ìš´ë”© ë°•ìŠ¤ ë‚´ ì˜ì—­ì„ ë¸”ëŸ¬ ì²˜ë¦¬ í˜¸ì¶œ
            self.blur_bounding_box_areas(video_path)

        elif save_input == 'n':
            print("Video not saved.")
        else:
            print("Invalid input. Please enter 'y' or 'n'.")
            self.save_video_prompt()

    def blur_bounding_box_areas(self, video_path):
        
        #ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ì„ ë¸”ëŸ¬ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ
        output_name = os.path.basename(video_path).replace(".webm", "_blurred.webm")
        output_path = os.path.join(self.output_dir, output_name)
        
        # Video reading and processing
        cap = cv2.VideoCapture(video_path)
        fourcc = cv2.VideoWriter_fourcc(*'VP80') # WebM ì½”ë± (VP80), MP4 ì½”ë± (mp4v)
        out = cv2.VideoWriter(output_path, fourcc, 30, (int(cap.get(3)), int(cap.get(4))))
        
        for i in range(len(self.frames)):
            ret, frame = cap.read()
            if not ret:
                break

            boxes = self.boxes[i]  # í•´ë‹¹ í”„ë ˆì„ì˜ ë°•ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            for box in boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ì„ íë¦¬ê²Œ ì²˜ë¦¬
                roi = frame[y1:y2, x1:x2]
                blurred_roi = cv2.GaussianBlur(roi, (15, 15), 0)
                frame[y1:y2, x1:x2] = blurred_roi

            out.write(frame)

        cap.release()
        out.release()
        print(f"Blurred video saved at {output_path}")



### Video
if __name__ == '__main__':
    source = "data/street.webm"
    tracker = PersonTracker(model_path='model/yolo11n.pt')
    tracker.detect_and_track(source=source)

### WebCam
#if __name__ == '__main__':
#    source = 0  # Use 0 for the default webcam
#    tracker = PersonTracker(model_path='model/yolo11n.pt')
#    tracker.detect_and_track(source=source)
